<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Super-Resolution for medical imaging | Hendrik&#39;s Portfolio</title>
<meta name="keywords" content="">
<meta name="description" content="Leveraging diffusion models for super-resolution tasks in MRI.
Demo Low Resolution 1.5T to High Resolution 3T MRI Overview Latest diffusion models have shown promising results in super-resolution tasks. This project aims to leverage these models to enhance the resolution of MRI images. It involves training a diffusion model on HR / LR MRI pairs to learn the noise distribution and generate high-resolution images from low-resolution inputs. Using a cascade of these models, we can achieve significant improvements in image quality.">
<meta name="author" content="">
<link rel="canonical" href="https://chichonnade.github.io/posts/mri-super-resolution/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.fc220c15db4aef0318bbf30adc45d33d4d7c88deff3238b23eb255afdc472ca6.css" integrity="sha256-/CIMFdtK7wMYu/MK3EXTPU18iN7/MjiyPrJVr9xHLKY=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://chichonnade.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://chichonnade.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://chichonnade.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://chichonnade.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://chichonnade.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://chichonnade.github.io/posts/mri-super-resolution/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
  

<meta property="og:title" content="Super-Resolution for medical imaging" />
<meta property="og:description" content="Leveraging diffusion models for super-resolution tasks in MRI.
Demo Low Resolution 1.5T to High Resolution 3T MRI Overview Latest diffusion models have shown promising results in super-resolution tasks. This project aims to leverage these models to enhance the resolution of MRI images. It involves training a diffusion model on HR / LR MRI pairs to learn the noise distribution and generate high-resolution images from low-resolution inputs. Using a cascade of these models, we can achieve significant improvements in image quality." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://chichonnade.github.io/posts/mri-super-resolution/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-08-21T00:00:00+00:00" />
<meta property="article:modified_time" content="2024-08-21T00:00:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Super-Resolution for medical imaging"/>
<meta name="twitter:description" content="Leveraging diffusion models for super-resolution tasks in MRI.
Demo Low Resolution 1.5T to High Resolution 3T MRI Overview Latest diffusion models have shown promising results in super-resolution tasks. This project aims to leverage these models to enhance the resolution of MRI images. It involves training a diffusion model on HR / LR MRI pairs to learn the noise distribution and generate high-resolution images from low-resolution inputs. Using a cascade of these models, we can achieve significant improvements in image quality."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://chichonnade.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Super-Resolution for medical imaging",
      "item": "https://chichonnade.github.io/posts/mri-super-resolution/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Super-Resolution for medical imaging",
  "name": "Super-Resolution for medical imaging",
  "description": "Leveraging diffusion models for super-resolution tasks in MRI.\nDemo Low Resolution 1.5T to High Resolution 3T MRI Overview Latest diffusion models have shown promising results in super-resolution tasks. This project aims to leverage these models to enhance the resolution of MRI images. It involves training a diffusion model on HR / LR MRI pairs to learn the noise distribution and generate high-resolution images from low-resolution inputs. Using a cascade of these models, we can achieve significant improvements in image quality.",
  "keywords": [
    
  ],
  "articleBody": " Leveraging diffusion models for super-resolution tasks in MRI.\nDemo Low Resolution 1.5T to High Resolution 3T MRI Overview Latest diffusion models have shown promising results in super-resolution tasks. This project aims to leverage these models to enhance the resolution of MRI images. It involves training a diffusion model on HR / LR MRI pairs to learn the noise distribution and generate high-resolution images from low-resolution inputs. Using a cascade of these models, we can achieve significant improvements in image quality.\nForward noise process Using a cosine noise schedule, we can gradually add noise to the input image, allowing the model to learn the noise distribution and generate high-resolution images. The noise variance is increased over 1000 steps with a maximum variance of 0.3 at the final step to prevent adding out-of-bound noise values.\nHere are the formulas used:\nCosine Schedule for Alphas: $$ \\alpha_t = \\cos^2\\left(\\frac{\\pi t}{2T}\\right) $$ Cumulative Product of Alphas: $$\\bar{\\alpha_{t}} = \\prod_{i=1}^{t}\\alpha_i$$ Forward Noise Process: $$ q(x_t | x_0) = \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha}_t} x_0, 0.3(1 - \\bar{\\alpha}_t) I) $$ \\( x_t \\) can be sampled as: \\[ x_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{(1 - \\bar{\\alpha}_t)} \\varepsilon_{t}, \\text{ where } \\varepsilon_{t} \\sim \\mathcal{N}(0, \\mathbf{0.3I}) \\]Where:\n\\( \\alpha_t \\) represents the amount of signal retained at time step \\( t \\). \\( \\bar{\\alpha}_t \\) is the cumulative product of the alpha values up to time step \\( t \\). \\( x_t \\) is the noisy image at step \\( t \\). \\( x_0 \\) is the original image. \\( q(x_t \\mid x_0) \\) represents the probability distribution of the noisy image given the original image. LR/HR pairs We use the Human Connectome Project (HCP) dataset, which contains both 1.5T and 3T MRI scans. The LR images are generated by downsampling the HR images using the spline interpolation method. Then we train the diffusion model on these LR/HR pairs to learn the noise distribution and generate high-resolution images.\nReverse process using a 3D-Unet Our model is based on the original DDPM architecture with 3D convolutions to handle the 3D MRI data. The model consists of a series of diffusion steps, each with a series of 3D convolutions, normalization layers, and a reversible block.\nReverse Noise Schedule for Betas: $$ \\beta_t = \\frac{1 - \\alpha_{t+1}}{1 - \\bar{\\alpha}_{t+1}} $$ Variance of the Reverse Process: $$ \\sigma_t^2 = \\frac{1 - \\bar{\\alpha}_t}{1 - \\bar{\\alpha}_{t+1}} \\beta_t $$ Reverse Process Sampling: $$ p(x_{t-1} \\mid x_t, x_0) = \\mathcal{N}(x_{t-1}; \\mu_t(x_t, x_0), \\sigma_t^2 I) $$ Mean of the Reverse Process: $$ \\mu_t(x_t, x_0) = \\frac{\\sqrt{\\bar{\\alpha}_{t-1}} \\beta_t}{1 - \\bar{\\alpha}_t} x_0 + \\frac{\\sqrt{\\alpha_t}(1 - \\bar{\\alpha}_{t-1})}{1 - \\bar{\\alpha}_t} x_t $$ Where:\n\\( \\beta_t \\) represents the noise variance for the reverse process at time step \\( t \\). \\( \\sigma_t^2 \\) is the variance of the reverse process, which determines how much noise is reduced at each reverse step. \\( x_t \\) is the noisy image at step \\( t \\). \\( x_{t-1} \\) is the denoised image at the previous step \\( t-1 \\). \\( \\mu_t(x_t, x_0) \\) is the mean of the reverse process, guiding the denoising from \\( x_t \\) to \\( x_{t-1} \\). \\( p(x_{t-1} \\mid x_t, x_0) \\) represents the probability distribution of the denoised image at step \\( t-1 \\) given the noisy image at step \\( t \\) and the original image \\( x_0 \\). Contents of the Repository Project Structure Installation Usage Data Sample Data Utilities License Project Structure data/: Contains the datasets used in the project.\nBrats2020/: Dataset related to the BraTS 2020 competition, focusing on brain tumor segmentation. HCP/: Dataset from the Human Connectome Project (HCP), used for MRI data. MNIST/: Contains the MNIST dataset, possibly used for experimentation or model testing. sample_data/: Contains sample data for testing or demo purposes.\nStructural Preprocessed for 7T (1.6mm/59k mesh): Preprocessed structural data intended for use with 7T MRI resolution. 100610_3T_Structural_1.6mm_preproc.zip: A ZIP file containing the preprocessed data for 3T MRI. 100610_3T_Structural_1.6mm_preproc.zip.md5: MD5 checksum for the ZIP file to verify its integrity. utils/: Utility scripts and notebooks for the project.\nDDPM.ipynb: A Jupyter notebook for training or experimenting with the DDPM model. diffusion_model_mnist.pth: A pre-trained diffusion model on the MNIST dataset. eda_notebook.ipynb: An exploratory data analysis (EDA) notebook. me.jpg: An image file, possibly a personal photo or a sample image for testing. mri_slices_combined.avi: Another video file, likely related to MRI data visualization. plot_t1w.py: A Python script for plotting T1-weighted MRI images. venv/: Virtual environment for managing project dependencies.\nInstallation To set up the environment for this project:\nClone the repository: ```bash git clone https://github.com/your-username/MRI-Super-Resolution.git ``` Navigate to the project directory: ```bash cd MRI-Super-Resolution ``` Create a virtual environment: ```bash python3 -m venv venv ``` Activate the virtual environment: ```bash source venv/bin/activate # On Windows: venv\\Scripts\u0007ctivate ``` Install the required packages: ```bash pip install -r requirements.txt ``` Usage Instructions on how to run the notebooks, train models, and process the MRI data:\nExploratory Data Analysis: Use eda_notebook.ipynb to explore the dataset before training.\nVisualization: Run plot_t1w.py to generate visualizations of T1-weighted MRI images.\nSample Data Sample data under sample_data/Structural Preprocessed for 7T is provided to test the pipeline without downloading full datasets.\nUtilities Scripts and notebooks: Utility files provided in the utils/ directory to assist with model training, data exploration, and visualization. License This project is licensed under the MIT License - see the LICENSE file for details.\n",
  "wordCount" : "882",
  "inLanguage": "en",
  "datePublished": "2024-08-21T00:00:00Z",
  "dateModified": "2024-08-21T00:00:00Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://chichonnade.github.io/posts/mri-super-resolution/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Hendrik's Portfolio",
    "logo": {
      "@type": "ImageObject",
      "url": "https://chichonnade.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://chichonnade.github.io/" accesskey="h" title="Hendrik&#39;s Portfolio (Alt + H)">Hendrik&#39;s Portfolio</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Super-Resolution for medical imaging
    </h1>
    <div class="post-meta"><span title='2024-08-21 00:00:00 +0000 UTC'>August 21, 2024</span>

</div>
  </header> 
  <div class="post-content">
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<p>Leveraging diffusion models for super-resolution tasks in MRI.</p>
<h2 id="demo">Demo<a hidden class="anchor" aria-hidden="true" href="#demo">#</a></h2>
<h3 id="low-resolution-15t-to-high-resolution-3t-mri">Low Resolution 1.5T to High Resolution 3T MRI<a hidden class="anchor" aria-hidden="true" href="#low-resolution-15t-to-high-resolution-3t-mri">#</a></h3>
<div style="text-align: center">
    
   <img src="assets/mri_slice_LR.gif" alt="Low Resolution MRI" width="800" style="display: block; margin-left: auto; margin-right: auto;" />
   <img src="assets/mri_slice_HR.gif" alt="High Resolution MRI" width="800" style="display: block; margin-left: auto; margin-right: auto;" />

</div>
<h2 id="overview">Overview<a hidden class="anchor" aria-hidden="true" href="#overview">#</a></h2>
<p>Latest diffusion models have shown promising results in super-resolution tasks. This project aims to leverage these models to enhance the resolution of MRI images. It involves training a diffusion model on HR / LR MRI pairs to learn the noise distribution and generate high-resolution images from low-resolution inputs. Using a cascade of these models, we can achieve significant improvements in image quality.</p>
<h2 id="forward-noise-process">Forward noise process<a hidden class="anchor" aria-hidden="true" href="#forward-noise-process">#</a></h2>
<p><img src="assets/cosine_noise_scheduler.png" alt="High Resolution MRI" width="800" style="display: block; margin-left: auto; margin-right: auto;" />
Using a cosine noise schedule, we can gradually add noise to the input image, allowing the model to learn the noise distribution and generate high-resolution images. The noise variance is increased over 1000 steps with a maximum variance of 0.3 at the final step to prevent adding out-of-bound noise values.</p>
<p>Here are the formulas used:</p>
<ol>
<li>
<p><strong>Cosine Schedule for Alphas</strong>:
</p>
$$ \alpha_t = \cos^2\left(\frac{\pi t}{2T}\right) $$</li>
<li>
<p><strong>Cumulative Product of Alphas</strong>:
</p>
$$\bar{\alpha_{t}} = \prod_{i=1}^{t}\alpha_i$$</li>
<li>
<p><strong>Forward Noise Process</strong>:
</p>
$$ q(x_t | x_0) = \mathcal{N}(x_t; \sqrt{\bar{\alpha}_t} x_0, 0.3(1 - \bar{\alpha}_t) I) $$</li>
</ol>
<p>\( x_t \) can be sampled as: </p>
\[ x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{(1 - \bar{\alpha}_t)} \varepsilon_{t}, \text{ where } \varepsilon_{t} \sim \mathcal{N}(0, \mathbf{0.3I}) \]<p>Where:</p>
<ul>
<li>\( \alpha_t \) represents the amount of signal retained at time step \( t \).</li>
<li>\( \bar{\alpha}_t \) is the cumulative product of the alpha values up to time step \( t \).</li>
<li>\( x_t \) is the noisy image at step \( t \).</li>
<li>\( x_0 \) is the original image.</li>
<li>\( q(x_t \mid x_0) \) represents the probability distribution of the noisy image given the original image.</li>
</ul>
<h2 id="lrhr-pairs">LR/HR pairs<a hidden class="anchor" aria-hidden="true" href="#lrhr-pairs">#</a></h2>
<p>We use the Human Connectome Project (HCP) dataset, which contains both 1.5T and 3T MRI scans. The LR images are generated by downsampling the HR images using the spline interpolation method. Then we train the diffusion model on these LR/HR pairs to learn the noise distribution and generate high-resolution images.</p>
<h2 id="reverse-process-using-a-3d-unet">Reverse process using a 3D-Unet<a hidden class="anchor" aria-hidden="true" href="#reverse-process-using-a-3d-unet">#</a></h2>
<div style="text-align: center">
    
   <img src="assets/3DUnetSuperRes.png" alt="Low Resolution MRI" width="600" style="display: block; margin-left: auto; margin-right: auto;" />

</div>
<p>Our model is based on the original DDPM architecture with 3D convolutions to handle the 3D MRI data. The model consists of a series of diffusion steps, each with a series of 3D convolutions, normalization layers, and a reversible block.</p>
<ol>
<li>
<p><strong>Reverse Noise Schedule for Betas</strong>:
</p>
$$ \beta_t = \frac{1 - \alpha_{t+1}}{1 - \bar{\alpha}_{t+1}} $$</li>
<li>
<p><strong>Variance of the Reverse Process</strong>:
</p>
$$ \sigma_t^2 = \frac{1 - \bar{\alpha}_t}{1 - \bar{\alpha}_{t+1}} \beta_t $$</li>
<li>
<p><strong>Reverse Process Sampling</strong>:
</p>
$$ p(x_{t-1} \mid x_t, x_0) = \mathcal{N}(x_{t-1}; \mu_t(x_t, x_0), \sigma_t^2 I) $$</li>
<li>
<p><strong>Mean of the Reverse Process</strong>:
</p>
$$ \mu_t(x_t, x_0) = \frac{\sqrt{\bar{\alpha}_{t-1}} \beta_t}{1 - \bar{\alpha}_t} x_0 + \frac{\sqrt{\alpha_t}(1 - \bar{\alpha}_{t-1})}{1 - \bar{\alpha}_t} x_t $$</li>
</ol>
<p>Where:</p>
<ul>
<li>\( \beta_t \) represents the noise variance for the reverse process at time step \( t \).</li>
<li>\( \sigma_t^2 \) is the variance of the reverse process, which determines how much noise is reduced at each reverse step.</li>
<li>\( x_t \) is the noisy image at step \( t \).</li>
<li>\( x_{t-1} \) is the denoised image at the previous step \( t-1 \).</li>
<li>\( \mu_t(x_t, x_0) \) is the mean of the reverse process, guiding the denoising from \( x_t \) to \( x_{t-1} \).</li>
<li>\( p(x_{t-1} \mid x_t, x_0) \) represents the probability distribution of the denoised image at step \( t-1 \) given the noisy image at step \( t \) and the original image \( x_0 \).</li>
</ul>
<h2 id="contents-of-the-repository">Contents of the Repository<a hidden class="anchor" aria-hidden="true" href="#contents-of-the-repository">#</a></h2>
<ul>
<li><a href="#project-structure">Project Structure</a></li>
<li><a href="#installation">Installation</a></li>
<li><a href="#usage">Usage</a></li>
<li><a href="#data">Data</a></li>
<li><a href="#sample-data">Sample Data</a></li>
<li><a href="#utilities">Utilities</a></li>
<li><a href="#license">License</a></li>
</ul>
<h2 id="project-structure">Project Structure<a hidden class="anchor" aria-hidden="true" href="#project-structure">#</a></h2>
<ul>
<li>
<p><strong>data/</strong>: Contains the datasets used in the project.</p>
<ul>
<li><strong>Brats2020/</strong>: Dataset related to the BraTS 2020 competition, focusing on brain tumor segmentation.</li>
<li><strong>HCP/</strong>: Dataset from the Human Connectome Project (HCP), used for MRI data.</li>
<li><strong>MNIST/</strong>: Contains the MNIST dataset, possibly used for experimentation or model testing.</li>
</ul>
</li>
<li>
<p><strong>sample_data/</strong>: Contains sample data for testing or demo purposes.</p>
<ul>
<li><strong>Structural Preprocessed for 7T (1.6mm/59k mesh)</strong>: Preprocessed structural data intended for use with 7T MRI resolution.
<ul>
<li><code>100610_3T_Structural_1.6mm_preproc.zip</code>: A ZIP file containing the preprocessed data for 3T MRI.</li>
<li><code>100610_3T_Structural_1.6mm_preproc.zip.md5</code>: MD5 checksum for the ZIP file to verify its integrity.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>utils/</strong>: Utility scripts and notebooks for the project.</p>
<ul>
<li><code>DDPM.ipynb</code>: A Jupyter notebook for training or experimenting with the DDPM model.</li>
<li><code>diffusion_model_mnist.pth</code>: A pre-trained diffusion model on the MNIST dataset.</li>
<li><code>eda_notebook.ipynb</code>: An exploratory data analysis (EDA) notebook.</li>
<li><code>me.jpg</code>: An image file, possibly a personal photo or a sample image for testing.</li>
<li><code>mri_slices_combined.avi</code>: Another video file, likely related to MRI data visualization.</li>
<li><code>plot_t1w.py</code>: A Python script for plotting T1-weighted MRI images.</li>
</ul>
</li>
<li>
<p><strong>venv/</strong>: Virtual environment for managing project dependencies.</p>
</li>
</ul>
<h2 id="installation">Installation<a hidden class="anchor" aria-hidden="true" href="#installation">#</a></h2>
<p>To set up the environment for this project:</p>
<ol>
<li>Clone the repository:
```bash
git clone <a href="https://github.com/your-username/MRI-Super-Resolution.git">https://github.com/your-username/MRI-Super-Resolution.git</a>
```</li>
<li>Navigate to the project directory:
```bash
cd MRI-Super-Resolution
```</li>
<li>Create a virtual environment:
```bash
python3 -m venv venv
```</li>
<li>Activate the virtual environment:
```bash
source venv/bin/activate  # On Windows: venv\Scriptsctivate
```</li>
<li>Install the required packages:
```bash
pip install -r requirements.txt
```</li>
</ol>
<h2 id="usage">Usage<a hidden class="anchor" aria-hidden="true" href="#usage">#</a></h2>
<p>Instructions on how to run the notebooks, train models, and process the MRI data:</p>
<p><strong>Exploratory Data Analysis:</strong>
Use <code>eda_notebook.ipynb</code> to explore the dataset before training.</p>
<p><strong>Visualization:</strong>
Run <code>plot_t1w.py</code> to generate visualizations of T1-weighted MRI images.</p>
<h2 id="sample-data">Sample Data<a hidden class="anchor" aria-hidden="true" href="#sample-data">#</a></h2>
<p>Sample data under <code>sample_data/Structural Preprocessed for 7T</code> is provided to test the pipeline without downloading full datasets.</p>
<h2 id="utilities">Utilities<a hidden class="anchor" aria-hidden="true" href="#utilities">#</a></h2>
<ul>
<li><strong>Scripts and notebooks</strong>: Utility files provided in the <code>utils/</code> directory to assist with model training, data exploration, and visualization.</li>
</ul>
<h2 id="license">License<a hidden class="anchor" aria-hidden="true" href="#license">#</a></h2>
<p>This project is licensed under the MIT License - see the LICENSE file for details.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2024 <a href="https://chichonnade.github.io/">Hendrik&#39;s Portfolio</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
